<div align="center">

# ğŸ¯ AI Career Coach

### *Intelligent Resume-Job Matching with Vector Embeddings*

[![Python](https://img.shields.io/badge/Python-3.8+-blue.svg)](https://www.python.org/)
[![ChromaDB](https://img.shields.io/badge/ChromaDB-Vector%20DB-orange.svg)](https://www.trychroma.com/)
[![Docker](https://img.shields.io/badge/Docker-Ready-2496ED.svg)](https://www.docker.com/)
[![License](https://img.shields.io/badge/License-Academic-green.svg)](#)

**An AI-powered career coaching platform using semantic search to match resumes with job opportunities**

[Quick Start](#-quick-start) â€¢ [Features](#-features) â€¢ [Docker Setup](#-docker-setup) â€¢ [Documentation](#-usage)

</div>

---

## ğŸŒŸ Features

<table>
<tr>
<td width="50%">

### ğŸ” **Semantic Matching**
- Vector embeddings for intelligent matching
- 1000+ resumes across 9 categories
- 2,277+ job descriptions
- Sub-100ms query performance

</td>
<td width="50%">

### ğŸš€ **Easy Setup**
- One-command automated setup
- Docker containerization
- Persistent local storage
- No external dependencies

</td>
</tr>
<tr>
<td width="50%">

### ğŸ“Š **Smart Filtering**
- Category-based search
- Similarity scoring
- Metadata filtering
- Ranking & recommendations

</td>
<td width="50%">

### ğŸ› ï¸ **Developer Friendly**
- Simple Python API
- FastAPI backend ready
- Gradio UI included
- Comprehensive documentation

</td>
</tr>
</table>

---

## ğŸ“ Project Structure

```
GenAiProject/
â”œâ”€â”€ ğŸ”§ Backend/              # API and business logic
â”œâ”€â”€ ğŸ¨ Frontend/             # Gradio user interface  
â”œâ”€â”€ ğŸ§  Rag/                  # Vector Database & RAG pipeline
â”‚   â”œâ”€â”€ chroma_setup.py                # Initialize ChromaDB
â”‚   â”œâ”€â”€ chroma_ingestion.py            # Embed & store documents
â”‚   â”œâ”€â”€ extract_resumes.py             # PDF text extraction
â”‚   â”œâ”€â”€ career_coach_matcher.py        # Matching API
â”‚   â””â”€â”€ quickstart.py                  # Automated setup
â”œâ”€â”€ ğŸ’¾ Data/                 # Data storage
â”‚   â”œâ”€â”€ chromadb/                      # Vector database
â”‚   â”œâ”€â”€ resumes_extracted.csv          # Extracted resume texts
â”‚   â”œâ”€â”€ Job_Descriptions/              # Cleaned job CSVs
â”‚   â””â”€â”€ raw/cv_samples/                # 1000+ PDF resumes
â”œâ”€â”€ ğŸ³ docker-compose.yml    # Docker orchestration
â””â”€â”€ ğŸ“„ readme.md             # This file
```

## ğŸš€ Quick Start

### 1. Install Dependencies

```powershell
cd "path\to\Gen_AI_Career_Coach"
pip install -r Rag\requirements_chroma.txt
```

### 2. Automated Setup (Recommended)

```powershell
python Rag\quickstart.py
```

This will:

### 3. Manual Setup (Step-by-step)

```powershell
# Initialize ChromaDB
python Rag\chroma_setup.py

# Extract resume text from PDFs
python Rag\extract_resumes.py

# Ingest documents into ChromaDB
python Rag\chroma_ingestion.py
```

## ğŸ³ Dockerized Setup

> **Prerequisites:**
> - Docker Desktop 4.27+ with Docker Compose v2
> - `Data/` directory initialized (run `python Rag/quickstart.py` once on the host or inside the container via `docker compose run --rm app python Rag/quickstart.py`)

```powershell
# Build images and start the full stack (Gradio app + Ollama)
docker compose up --build

# Optional: run ingestion/maintenance tasks inside the container
docker compose run --rm app python Rag\quickstart.py
```

What this does:

- Spins up the main application container exposing Gradio on `http://localhost:7860`
- Starts an `ollama/ollama` container, automatically pulls the **mistral:7b-instruct-q4_K_M** model (fits in ~4â€¯GiB RAM), and shares the model cache via a named volume
- Mounts your local `Data/` folder into the container so the ChromaDB index and CSV assets stay persistent between runs

Environment knobs:

- `PORT` â€“ externalize the Gradio port (default `7860`)
- `GRADIO_SERVER_NAME` â€“ change the bind address (defaults to `0.0.0.0` inside Docker)
- `OLLAMA_MODEL` â€“ override the Ollama model without code changes (default `mistral:7b-instruct-q4_K_M`)

## ğŸ’¾ Vector Database: ChromaDB

### Why ChromaDB?

- **Lightweight** - No server setup required
- **Fast** - HNSW indexing for sub-100ms queries
- **Easy** - Simple Python API
- **Flexible** - Metadata filtering and custom metadata
- **Persistent** - Local storage for your project

### Database Contents

| Collection | Documents | Source |
|-----------|-----------|--------|
| `resumes` | 1000+ | PDF files in `Data/raw/cv_samples/` |
| `job_descriptions` | 2,277 | Cleaned CSVs from `Data/Job_Descriptions/` |

### Categories

Resumes are organized in 9 professional categories:
- `ENGINEERING`
- `FINANCE`
- `FITNESS`
- `HEALTHCARE`
- `HR`
- `INFORMATION-TECHNOLOGY`
- `PUBLIC-RELATIONS`
- `SALES`
- `TEACHER`

## ğŸ” Basic Usage

### Find Jobs for a Resume

```python
from Rag.career_coach_matcher import CareerCoachMatcher

matcher = CareerCoachMatcher()

# Find matching jobs
resume_text = "Senior Software Engineer with Python experience..."
jobs = matcher.find_jobs_for_resume(resume_text, n_results=10)

for job in jobs:
    print(f"â€¢ {job.metadata['job_title']} ({job.similarity_score:.1%})")
```

### Find Resumes for a Job

```python
# Find matching resumes
resumes = matcher.find_resumes_for_job(
    job_title="Senior Cloud Engineer",
    job_description="Looking for AWS expert...",
    n_results=10,
    category_filter="INFORMATION-TECHNOLOGY"
)

for resume in resumes:
    print(f"â€¢ {resume.metadata['category']} ({resume.similarity_score:.1%})")
```

### Get Database Statistics

```python
stats = matcher.get_db_stats()
print(f"Total Resumes: {stats['total_resumes']}")
print(f"Total Jobs: {stats['total_jobs']}")
print(f"Embedding Model: {stats['embedding_model']}")
```

## ğŸ§  Embedding Model

**Model**: `all-MiniLM-L6-v2`
- **Dimensions**: 384
- **Speed**: Fast (~1000 documents/min)
- **Task**: Semantic search
- **Accuracy**: Excellent for career matching

**Storage Requirements**:
- Resumes: ~2-3 MB
- Jobs: ~1-2 MB
- Total: ~5-7 MB (highly compressed)

## ğŸ“Š Data Pipeline

```
Input Data
â”œâ”€â”€ Resume PDFs (1000+)        â†’ Extract Text
â”œâ”€â”€ Job Description CSVs        â†’ Clean Data
â”‚
â†“
Generate Embeddings
â”œâ”€â”€ Model: sentence-transformers (384-dim)
â”œâ”€â”€ Batch Processing: 32 documents/batch
â”‚
â†“
Store in ChromaDB
â”œâ”€â”€ Resumes Collection
â”œâ”€â”€ Jobs Collection
â”‚
â†“
Query & Search
â”œâ”€â”€ Similarity Search
â”œâ”€â”€ Category Filtering
â”œâ”€â”€ Ranking & Scoring
```

## ğŸ“ Data Cleaning

All data has been cleaned and prepared:

- **job_title_des_cleaned.csv** (2,277 records)
  - Columns: Job Title, Job Description
  - HTML removed âœ“
  - Ready for embeddings âœ“

- **job_descriptions_2_cleaned.csv.gz** (30,002 records)
  - Compressed: 38.73 MB (67% reduction)
  - GitHub-compliant âœ“

## ğŸ”§ Configuration

Edit `Rag/chroma_ingestion.py` to customize:

```python
# Embedding model
EMBEDDING_MODEL = "all-MiniLM-L6-v2"  # Change for different accuracy/speed tradeoff

# Batch processing
BATCH_SIZE = 32  # Adjust for memory constraints

# Database location
CHROMA_DB_PATH = Path(__file__).parent.parent / "Data" / "chromadb"
```

## ğŸ“š Advanced Features

### Metadata Filtering

```python
# Query only IT resumes
resumes = matcher.find_resumes_for_job(
    job_title="DevOps Engineer",
    job_description="Kubernetes, AWS...",
    category_filter="INFORMATION-TECHNOLOGY"
)
```

### Minimum Similarity Threshold

```python
# Only show highly similar matches (> 70%)
jobs = matcher.find_jobs_for_resume(
    resume_text="...",
    min_score=0.7
)
```

### Direct ChromaDB Access

```python
from chroma_setup import get_or_create_db

client, resumes_col, jobs_col = get_or_create_db()

# Custom query
results = jobs_col.query(
    query_embeddings=[...],
    n_results=20,
    where={"job_title": {"$contains": "Engineer"}}
)
```

## ğŸ› Troubleshooting

**Q: "ModuleNotFoundError: No module named 'chromadb'"**
- A: Run `pip install -r Rag\requirements_chroma.txt`

**Q: "CUDA out of memory"**
- A: Embeddings use CPU by default. Force CPU in code:
  ```python
  embedder = ChromaEmbedder()
  # CPU is used automatically for small models
  ```

**Q: PDF extraction produces empty text**
- A: Some PDFs may be scanned images. Install OCR:
  ```powershell
  pip install pytesseract pillow
  ```

**Q: Reset ChromaDB data**
- A: Delete the database folder:
  ```powershell
  Remove-Item "Data\chromadb" -Recurse -Force
  python Rag\chroma_setup.py
  ```

## ğŸ“– Documentation

- **Setup Guide**: `Rag/CHROMA_SETUP.md`
- **Quick Start**: `Rag/quickstart.py`
- **Matcher API**: `Rag/career_coach_matcher.py`

## ğŸ“ Learning Resources

- [ChromaDB Documentation](https://docs.trychroma.com/)
- [Sentence Transformers](https://www.sbert.net/)
- [Vector Databases Guide](https://www.deepset.ai/blog/the-complete-guide-to-vector-databases)
- [Semantic Search](https://huggingface.co/blog/semantic-search)

## ğŸ“‹ Next Steps

- [ ] Build REST API (FastAPI/Flask)
- [ ] Create web frontend
- [ ] Implement skills extraction
- [ ] Add interview prep module
- [ ] Deploy to cloud (Azure)
- [ ] Add real-time resume updates

## ğŸ‘¥ Contributors

- Robin De Meyer
- Roan Vandemeulebroucke

## ğŸ“„ License

Project Information: AI Career Coach Application (2025-2026)

---

**Status**: âœ… Vector Database Setup Complete | Ready for Integration

Last Updated: November 19, 2025
